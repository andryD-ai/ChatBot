{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andryD-ai/ChatBot/blob/main/Gpt2_LLM_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGSBgxSJEfJj"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFBRn_Pb8w0j"
      },
      "source": [
        "##Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can load your dataset and then edit read_chat function in class DataLoaderLite\n",
        "# If your data is big, plz store and load in parts\n",
        "\n",
        "# Generate 10,000 customer service related question-answer pairs\n",
        "customer_service_questions = [\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "# Define corresponding answers for customer service questions\n",
        "customer_service_answers = [\n",
        "\n",
        "]\n"
      ],
      "metadata": {
        "id": "gys48wwBP7f8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS240Uc2UDie"
      },
      "source": [
        "##Transformer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "0IS-j4agN4-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbGwp6qrW1tv"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from dataclasses import dataclass\n",
        "import regex as re\n",
        "import string\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVfamCJ3Wwx5"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPT2Config():\n",
        "  vocab_size: int = 50257 #number of tokens: 50k tokens BPE + 256 byte tokens + 1 token <|endoftext|>\n",
        "  block_size: int = 1024  #max sequence length\n",
        "  n_head: int = 12  #number of head\n",
        "  n_layer: int = 12 #number of layer\n",
        "  n_embd: int = 768 #number of embedding dimension\n",
        "  device: str = \"cpu\"\n",
        "\n",
        "@dataclass\n",
        "class LoRAConfig():\n",
        "  rank:int = 8\n",
        "  alpha:int = 16\n",
        "  dropout:float = 0.2\n",
        "\n",
        "class DataLoaderLite():\n",
        "  def __init__(self, B:int, T:int, file_data_path:str):\n",
        "    self.B = B\n",
        "    self.T = T\n",
        "\n",
        "    #load token from file and store in memory\n",
        "    list_lines = self.read_chat(file_data_path)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "    #Add special token\n",
        "    max_vocab_id = tokenizer.vocab_size\n",
        "    tokenizer.special_tokens = {\n",
        "        \"<|startoftext|>\": max_vocab_id + 1,\n",
        "        \"<|separator|>\": max_vocab_id + 2,\n",
        "        \"<|endoftext|>\": max_vocab_id + 3,\n",
        "        \"<|unk|>\": max_vocab_id + 4\n",
        "    }\n",
        "    self.tokens = []\n",
        "\n",
        "    print(\"Encode tokens:\")\n",
        "    for line in tqdm(list_lines):\n",
        "      tokens = tokenizer.encode(line)\n",
        "      self.tokens += tokens\n",
        "    self.tokens = torch.tensor(self.tokens)\n",
        "\n",
        "    print(len(self.tokens))\n",
        "\n",
        "    print(f\"Loaded {len(self.tokens)} from {file_data_path}\")\n",
        "    print(f\"1 epoch: {len(self.tokens) // (B*T)} batch\")\n",
        "\n",
        "    self.current_position = 0\n",
        "\n",
        "  def read_chat(self, file_path: str):\n",
        "\n",
        "    print(\"Reading data text and filtering\")\n",
        "    # with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    #     lines = f.readlines()\n",
        "\n",
        "    # Apply filters to remove unwanted lines\n",
        "    filtered_lines = []\n",
        "    tab_token = \"\\t\"\n",
        "    start_of_text_token = \"<|startoftext|>\"\n",
        "    end_of_text_token = \"<|endoftext|>\"\n",
        "    separator_token = \"<|separator|>\"\n",
        "    # for line in tqdm(lines):\n",
        "    #   if (bool(re.search(r'^[A-Za-z0-9,\\.?!\\t ]+$', line))):\n",
        "    #     line = line.replace(tab_token, separator_token).strip()\n",
        "    #     line = line + end_of_text_token\n",
        "    #     line = start_of_text_token + line\n",
        "    #     filtered_lines.append(line)\n",
        "\n",
        "    # in this test get data from custom list\n",
        "    for question, answer in zip(customer_service_questions, customer_service_answers):\n",
        "      filtered_lines.append(f\"{start_of_text_token}User{separator_token}{question}{end_of_text_token}{start_of_text_token}Assistant{separator_token}{answer}{end_of_text_token}\")\n",
        "\n",
        "    return filtered_lines\n",
        "\n",
        "  def next_batch(self):\n",
        "    B = self.B\n",
        "    T = self.T\n",
        "\n",
        "    buff = self.tokens[self.current_position: self.current_position+B*T+1]\n",
        "    x = buff[:-1].view(B,T) # input\n",
        "    y = buff[1:].view(B,T)  # output\n",
        "\n",
        "    # update possition in tensor\n",
        "    self.current_position += B*T\n",
        "\n",
        "    if self.current_position + B*T + 1 > len(self.tokens):\n",
        "      self.current_position = 0\n",
        "\n",
        "    return x, y\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    assert config.n_embd % config.n_head == 0\n",
        "    self.n_embd = config.n_embd\n",
        "    self.n_head = config.n_head\n",
        "\n",
        "    # key, query, value for all head in a batch\n",
        "    self.c_attn = nn.Linear(config.n_embd, config.n_embd*3)\n",
        "    # output projection\n",
        "    self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "    self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "    # calculate key, query and values for all heads in batch and move it forward to the batch dimension\n",
        "    # where nh is number of heads, hs is head size, and C is number of channel (equal nh * hs)\n",
        "    # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
        "    kqv = self.c_attn(x)\n",
        "    query, key, value = kqv.split(self.n_embd, dim = 2)\n",
        "    key = key.view(B, T, self.n_head, C // self.n_head).transpose(1,2)     # (B, nh, T, hs)\n",
        "    query = query.view(B, T, self.n_head, C // self.n_head).transpose(1,2) # (B, nh, T, hs)\n",
        "    value = value.view(B, T, self.n_head, C // self.n_head).transpose(1,2) # (B, nh, T, hs)\n",
        "\n",
        "    y = F.scaled_dot_product_attention(query, key, value, is_causal=True) # flash attention\n",
        "    y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "    # output projection\n",
        "    y = self.c_proj(y)\n",
        "\n",
        "    return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "    self.gelu = nn.GELU(approximate=\"tanh\")\n",
        "    self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "    self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.c_fc(x)\n",
        "    x = self.gelu(x)\n",
        "    x = self.c_proj(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, config) -> None:\n",
        "     super().__init__()\n",
        "\n",
        "     self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "     self.attn = Attention(config)\n",
        "     self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "     self.mlp = MLP(config)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.attn(self.ln_1(x))\n",
        "    x = x + self.mlp(self.ln_2(x))\n",
        "\n",
        "    return x\n",
        "\n",
        "class GPT2(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "\n",
        "    self.transformer = nn.ModuleDict(dict(\n",
        "      wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "      wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "      h = nn.ModuleList(Block(config) for _ in range(config.n_layer)),\n",
        "      ln_f = nn.LayerNorm(config.n_embd)\n",
        "    ))\n",
        "\n",
        "    self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "    # weight sharing scheme (like gpt2 pager)\n",
        "    self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "    #init params\n",
        "    self.apply(self._init_weight)\n",
        "\n",
        "  def _init_weight(self, module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "      std = 0.02\n",
        "      if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
        "        std *= (2 * self.config.n_layer) ** -0.5\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "      if module.bias is not None:\n",
        "        torch.nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module,nn.Embedding):\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "  def forward(self, x, targets=None):\n",
        "    B, T = x.size()\n",
        "    assert T <= self.config.block_size, f\"The length of the squence is {T}, is over block size {self.config.block_size}\"\n",
        "\n",
        "    #possisition and token embdding\n",
        "    pos = torch.arange(0, T, dtype=torch.long, device=x.device) # (T)\n",
        "    pos_embd = self.transformer.wpe(pos)\n",
        "    tok_embd = self.transformer.wte(x)\n",
        "    x = pos_embd + tok_embd\n",
        "\n",
        "    #Block\n",
        "    for block in self.transformer.h:\n",
        "      x = block(x)\n",
        "\n",
        "    #Final layernorm and the classifier\n",
        "    x = self.transformer.ln_f(x)\n",
        "\n",
        "    logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "    #loss\n",
        "    loss = 0\n",
        "\n",
        "    if targets is not None:\n",
        "      loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  @classmethod\n",
        "  def from_pretrained(cls, model_type):\n",
        "    assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "\n",
        "    from transformers import GPT2LMHeadModel\n",
        "    print(f\"Loading weights from pretrained gpt model {model_type}\")\n",
        "\n",
        "    # n_layer, n_head and n_embd are determined from model_type\n",
        "    config_args = {\n",
        "        'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "        'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "        'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "        'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "    }[model_type]\n",
        "\n",
        "    config_args['vocab_size'] = 50257\n",
        "    config_args['block_size'] = 1024\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else  \"cpu\"\n",
        "    config_args['device'] = device\n",
        "\n",
        "    config = GPT2Config(**config_args)\n",
        "    model = GPT2(config)\n",
        "\n",
        "    state_dict = model.state_dict()\n",
        "    sd_keys = state_dict.keys()\n",
        "    sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "    model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "    state_dict_hf = model_hf.state_dict()\n",
        "    sd_hf_keys = state_dict_hf.keys()\n",
        "    sd_hf_keys = [k for k in sd_hf_keys if not k.endswith('.attn.masked_bias')] #ignore buffer\n",
        "    sd_hf_keys = [k for k in sd_hf_keys if not k.endswith('.attn.bias')]   #ignore mask\n",
        "\n",
        "    assert len(sd_keys) == len(sd_hf_keys), f\"(Copy weights) Mismatched keys in model: new model - {len(sd_keys)} != pretrained model {len(sd_hf_keys)}\"\n",
        "\n",
        "    transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "    # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "    # this means that we have to transpose these weights when we import them\n",
        "    for k in sd_keys:\n",
        "      if any(k.endswith(ew) for ew in transposed):\n",
        "        # special treatment for the Conv1D weights we need to transpose\n",
        "        assert state_dict_hf[k].shape[::-1] == state_dict[k].shape, f\"state_dict[k] = {state_dict[k].shape}, state_dict_hf[k] = {state_dict_hf[k].shape} in {k}\"\n",
        "        with torch.no_grad():\n",
        "          state_dict[k].copy_(state_dict_hf[k].t())\n",
        "      else:\n",
        "        # vanilla copy over the other parameters\n",
        "        assert state_dict[k].shape == state_dict_hf[k].shape, f\"state_dict[k] = {state_dict[k].shape}, state_dict_hf[k] = {state_dict_hf[k].shape} in {k}\"\n",
        "        with torch.no_grad():\n",
        "          state_dict[k].copy_(state_dict_hf[k])\n",
        "\n",
        "    return model\n",
        "\n",
        "  def save_checkpoint(self,\n",
        "                      model,\n",
        "                      optimizer: torch.optim.Optimizer,\n",
        "                      epoch: int,\n",
        "                      loss: float,\n",
        "                      file_path: str = \"checkpoint.pth\"\n",
        "                      ) -> None:\n",
        "\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss\n",
        "    }\n",
        "    torch.save(checkpoint, file_path)\n",
        "\n",
        "  def model_train(self, data_loader:DataLoaderLite,  B:int, T:int, epoch:int):\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "    for step in range(epoch):\n",
        "      t0 = time.time()\n",
        "      optimizer.zero_grad()\n",
        "      x, y = data_loader.next_batch()\n",
        "      x, y = x.to(self.config.device), y.to(self.config.device)\n",
        "      with torch.autocast(device_type=self.config.device, dtype=torch.float16): # run in FP16\n",
        "        logit, loss = model(x, y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      torch.cuda.synchronize()\n",
        "      t1 = time.time()\n",
        "      tok_per_sec = (data_loader.B * data_loader.T)/(t1-t0)\n",
        "      print(f\"Step {step}, Loss: {loss.item()}, Speech: {(t1 - t0)*1000:.2f}ms, tok/sec: {tok_per_sec:.2f}\")\n",
        "      if ((step+1) % 100 == 0):\n",
        "        self.save_checkpoint(model, optimizer, step+1, loss, f\"/content/drive/MyDrive/Dataset/Chat_Dataset/Model_output/checkpoint{step+1}.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import replace\n",
        "import copy\n",
        "\n",
        "# LORA fine-turning\n",
        "class LoRALinear(nn.Module):\n",
        "  def __init__(self, linear:nn.Linear, config:LoRAConfig):\n",
        "    super().__init__()\n",
        "    # These are the weights from the original pretrained model\n",
        "    self.linear = linear\n",
        "    in_dim = linear.in_features\n",
        "    out_dim = linear.out_features\n",
        "    dropout = config.dropout\n",
        "    self.rank = config.rank\n",
        "    self.alpha = config.alpha\n",
        "\n",
        "    # These are the new LoRA params. In general rank << in_dim, out_dim\n",
        "    self.lora_a = nn.Linear(in_dim, self.rank, bias=False)\n",
        "    self.lora_b = nn.Linear(self.rank, out_dim, bias=False)\n",
        "\n",
        "    # Most implementations also include some dropout\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    # The original params are frozen, and only LoRA params are trainable.\n",
        "    self.lora_a.weight.requires_grad = True\n",
        "    self.lora_b.weight.requires_grad = True\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    # This would be the output of the original model\n",
        "    frozen_out = self.linear(x)\n",
        "\n",
        "    # lora_a projects inputs down to the much smaller self.rank,\n",
        "    # then lora_b projects back up to the output dimension\n",
        "    lora_out = self.lora_b(self.lora_a(self.dropout(x)))\n",
        "\n",
        "    # Finally, scale by the alpha parameter (normalized by rank)\n",
        "    # and add to the original model's outputs\n",
        "    return frozen_out + (self.alpha / self.rank) * lora_out\n",
        "\n",
        "class LoRAFineTuning():\n",
        "  @classmethod\n",
        "  def get_LoRA_model(cls, model:GPT2, config:LoRAConfig) -> GPT2:\n",
        "    lora_model = copy.deepcopy(model)\n",
        "\n",
        "    # freeze model\n",
        "    for name, param in lora_model.named_parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    LoRAFineTuning.replace_linear_layers_with_lora_layers(lora_model, config)\n",
        "\n",
        "    return lora_model\n",
        "\n",
        "  @classmethod\n",
        "  def replace_linear_layers_with_lora_layers(cls, module:nn.Module, config:LoRAConfig) -> None:\n",
        "    rank = config.rank\n",
        "    alpha = config.alpha\n",
        "    dropout = config.dropout\n",
        "\n",
        "    for name, child in list(module.named_children()):\n",
        "      if isinstance(child, nn.Linear):\n",
        "        setattr(module, name , LoRALinear(child, config))\n",
        "      else:\n",
        "         LoRAFineTuning.replace_linear_layers_with_lora_layers(child, config)\n",
        "\n",
        "  @classmethod\n",
        "  def print_trainable_parameters(cls, model: GPT2) -> None:\n",
        "    trainable_parameters = 0\n",
        "    all_parameters = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_parameters += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_parameters += param.numel()\n",
        "\n",
        "    print(\n",
        "        f\"All parameters: {all_parameters/1e6:.2f}M | \"\n",
        "        f\"Trainable parameters: {trainable_parameters/1e6:.2f}M | \"\n",
        "        f\"Trainable %: {100 * trainable_parameters / all_parameters:.2f}%\"\n",
        "    )"
      ],
      "metadata": {
        "id": "W-Qlc0_19VWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B = 8\n",
        "T = 1024\n",
        "epoch = 20\n",
        "# data_path = \"/content/drive/MyDrive/Dataset/Chat_Dataset/mo-customer-support-tweets-945k.txt\"\n",
        "data_loader = DataLoaderLite(B,T,file_data_path=\"\")\n"
      ],
      "metadata": {
        "id": "EnjNBoTOIs9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_float32_matmul_precision(\"high\") # run tf32 matrix multiplication if gpu supported\n",
        "model = GPT2.from_pretrained(\"gpt2\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "ccYJ0KQm7W3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Fine_turning model with LoRA\n",
        "# LoRA_config_arg = {\n",
        "#   \"rank\" : 64,\n",
        "#   \"alpha\" : 256,\n",
        "#   \"dropout\" : 0.2,\n",
        "# }\n",
        "\n",
        "# config = LoRAConfig(**LoRA_config_arg)\n",
        "# modeltest = copy.deepcopy(model)\n",
        "# modeltest = LoRAFineTuning.get_LoRA_model(modeltest, config)\n",
        "\n",
        "# LoRAFineTuning.print_trainable_parameters(modeltest)\n",
        "# LoRAFineTuning.print_trainable_parameters(model)\n"
      ],
      "metadata": {
        "id": "vqNDcRpeFRJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUG4JmH-EsOb"
      },
      "outputs": [],
      "source": [
        "# In this test we just fine-turning dataset and train with old weight from gpt2\n",
        "model.to(device)\n",
        "model = torch.compile(model)\n",
        "model.model_train(data_loader, B, T, 200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeFOaGiSiorp"
      },
      "outputs": [],
      "source": [
        "# Generate answer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "#Add special token\n",
        "max_vocab_id = tokenizer.vocab_size\n",
        "tokenizer.special_tokens = {\n",
        "    \"<|startoftext|>\": max_vocab_id + 1,\n",
        "    \"<|separator|>\": max_vocab_id + 2,\n",
        "    \"<|endoftext|>\": max_vocab_id + 3,\n",
        "    \"<|unk|>\": max_vocab_id + 4\n",
        "}\n",
        "\n",
        "text = \"<|startoftext|>User<|separator|>How much does this product cost?<|endoftext|>\"\n",
        "# text = \"<|startoftext|>User<|separator|>How are you today?<|endoftext|>\"\n",
        "text_repeat = 5\n",
        "max_length = 200\n",
        "\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "tokens = tokenizer.encode(text)\n",
        "tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "tokens = tokens.unsqueeze(0).repeat(text_repeat,1)\n",
        "tokens_gen = tokens.to(device)\n",
        "\n",
        "\n",
        "sample_rng = torch.Generator(device=device)\n",
        "sample_rng.manual_seed(42)\n",
        "num_end_gener = 0\n",
        "end_token = torch.tensor(tokenizer.encode(\"<|endoftext|>\")).to(device)\n",
        "\n",
        "while tokens_gen.size(1) < max_length:\n",
        "  with torch.no_grad():\n",
        "    logits, _ = model(tokens_gen, targets=None)\n",
        "    # take the logits at last position\n",
        "    logits = logits[:,-1,:] # (B, vocab_size)\n",
        "    # get the probabilities\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    # get top-k sampling of 50 (huggingface pipeline default)\n",
        "    # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "    topk_probs, topk_indices = torch.topk(probs, k=50, dim=-1)\n",
        "    # select a token from the top-k probabilities\n",
        "    # note: multinomial does not demand the input to sum to 1\n",
        "    ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
        "    # gather the corresponding indices\n",
        "    xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "    # append to the sequence\n",
        "    tokens_gen = torch.cat((tokens_gen, xcol), dim=1)\n",
        "  common = (xcol[:, None] == end_token).any(dim=1)\n",
        "  num_end_gener += sum(common)\n",
        "  if num_end_gener >= text_repeat:\n",
        "    break\n",
        "\n",
        "for i in range(text_repeat):\n",
        "  token = tokens_gen[i, :max_length].tolist()\n",
        "  decode = tokenizer.decode(token)\n",
        "  print(decode)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Connect with telegram bot\n"
      ],
      "metadata": {
        "id": "nA7n3IP7G3xm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "path_model = \"Your path\"\n",
        "\n",
        "pretrain_model = GPT2(GPT2Config)\n",
        "optimizer = torch.optim.AdamW(pretrain_model.parameters(), lr=3e-4)\n",
        "\n",
        "# 2. Load the checkpoint\n",
        "checkpoint = torch.load(path_model, map_location=device)\n",
        "\n",
        "# 3. Restore model and optimizer states\n",
        "# Strip the '_orig_mod.' prefix\n",
        "new_state_dict = {}\n",
        "for k, v in checkpoint['model_state_dict'].items():\n",
        "    new_key = k.replace(\"_orig_mod.\", \"\")  # Remove prefix\n",
        "    new_state_dict[new_key] = v\n",
        "\n",
        "# Load into model\n",
        "pretrain_model.load_state_dict(new_state_dict)\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "# 4. Optional: resume training from the last step/loss\n",
        "step = checkpoint['epoch']\n",
        "loss = checkpoint['loss']\n",
        "\n",
        "pretrain_model.eval()  # or model.train() depending on your use case\n",
        "pretrain_model.to(device)"
      ],
      "metadata": {
        "id": "35YGd8DQG7NT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate answer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "#Add special token\n",
        "max_vocab_id = tokenizer.vocab_size\n",
        "tokenizer.special_tokens = {\n",
        "    \"<|startoftext|>\": max_vocab_id + 1,\n",
        "    \"<|separator|>\": max_vocab_id + 2,\n",
        "    \"<|endoftext|>\": max_vocab_id + 3,\n",
        "    \"<|unk|>\": max_vocab_id + 4\n",
        "}\n",
        "\n",
        "\n",
        "request = \"How much does this product cost?\"\n",
        "\n",
        "def generate_response(request:str):\n",
        "  input = f\"<|startoftext|>User<|separator|>{request}<|endoftext|>\"\n",
        "  text_repeat = 1\n",
        "  max_length = 200\n",
        "\n",
        "  tokens = tokenizer.encode(input)\n",
        "  tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "  tokens = tokens.unsqueeze(0).repeat(text_repeat,1)\n",
        "  tokens_gen = tokens.to(device)\n",
        "\n",
        "  sample_rng = torch.Generator(device=device)\n",
        "  sample_rng.manual_seed(42)\n",
        "  num_end_gener = 0\n",
        "  end_token = torch.tensor(tokenizer.encode(\"<|endoftext|>\")).to(device)\n",
        "\n",
        "  while tokens_gen.size(1) < max_length:\n",
        "    with torch.no_grad():\n",
        "      logits, _ = pretrain_model(tokens_gen, targets=None)\n",
        "      # take the logits at last position\n",
        "      logits = logits[:,-1,:] # (B, vocab_size)\n",
        "      # get the probabilities\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      # get top-k sampling of 50 (huggingface pipeline default)\n",
        "      # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "      topk_probs, topk_indices = torch.topk(probs, k=50, dim=-1)\n",
        "      # select a token from the top-k probabilities\n",
        "      # note: multinomial does not demand the input to sum to 1\n",
        "      ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
        "      # gather the corresponding indices\n",
        "      xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "      # append to the sequence\n",
        "      tokens_gen = torch.cat((tokens_gen, xcol), dim=1)\n",
        "    common = (xcol[:, None] == end_token).any(dim=1)\n",
        "    num_end_gener += sum(common)\n",
        "    if num_end_gener >= text_repeat:\n",
        "      break\n",
        "\n",
        "  for i in range(text_repeat):\n",
        "    token = tokens_gen[i, :max_length].tolist()\n",
        "    decode = tokenizer.decode(token)\n",
        "    return decode\n"
      ],
      "metadata": {
        "id": "kjjIlEOhJwnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-telegram-bot"
      ],
      "metadata": {
        "id": "onSIiIivSDYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from telegram import Update\n",
        "from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, filters, ContextTypes\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Replace with your bot token\n",
        "BOT_TOKEN = 'token'\n",
        "\n",
        "# Start command handler\n",
        "async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    await update.message.reply_text(\"Hello! I'm your bot. How can I help you?\")\n",
        "\n",
        "# Close command handler\n",
        "async def close(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    await update.message.reply_text(\"I'm always here if you need help.\")\n",
        "\n",
        "# Echo handler (replies with the same message)\n",
        "async def echo(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    received_text = update.message.text\n",
        "    response = generate_response(received_text)\n",
        "    response = response.split(\"<|separator|>\")\n",
        "    response = response[-1].split(\"<|endoftext|>\")\n",
        "    await update.message.reply_text(response[0])\n",
        "\n",
        "# Main function to run the bot\n",
        "if __name__ == '__main__':\n",
        "    app = ApplicationBuilder().token(BOT_TOKEN).build()\n",
        "\n",
        "    app.add_handler(CommandHandler('start', start))\n",
        "    app.add_handler(CommandHandler('close', close))\n",
        "    app.add_handler(MessageHandler(filters.TEXT & (~filters.COMMAND), echo))\n",
        "\n",
        "    print(\"Bot is running...\")\n",
        "    app.run_polling()\n"
      ],
      "metadata": {
        "id": "MMZ-f_S_SBVg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}